{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241d7cb-f933-4aad-9815-c024de5551fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from latch.env.finger.finger import Finger\n",
    "\n",
    "from latch.models import ModelState\n",
    "\n",
    "from latch.rollout import eval_actor\n",
    "\n",
    "from latch.policy.actor_policy import ActorPolicy\n",
    "\n",
    "from latch.latch_config import LatchConfig\n",
    "from latch.config import TrainConfig, NetConfig, configure_state\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import jax\n",
    "from jax.tree_util import Partial\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import jax.experimental.host_callback\n",
    "\n",
    "from einops import einops, einsum, rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8460679",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig(\n",
    "    net_config=NetConfig(\n",
    "        latent_state_dim=6,\n",
    "        state_dim=6,\n",
    "        latent_action_dim=2,\n",
    "        action_dim=2,\n",
    "        latent_state_radius=1.5,\n",
    "        latent_action_radius=2.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Instantiate the environment\n",
    "env = Finger.init()\n",
    "\n",
    "# Initialize the train state\n",
    "train_state = configure_state(train_config=cfg, env=env)\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "\n",
    "checkpoint_path = Path(\"../../checkpoints/9wawoqrb/checkpoint_latest.zip\")\n",
    "\n",
    "shutil.unpack_archive(checkpoint_path, checkpoint_path.with_suffix(\"\"))\n",
    "train_state = checkpointer.restore(\n",
    "    checkpoint_path.with_suffix(\"\").absolute(), item=train_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aab5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(train_state, theta):\n",
    "    \"\"\"This evaluates the model and logs the results to wandb.\"\"\"\n",
    "    state_target = jnp.zeros(train_state.config.state_dim)\n",
    "    state_weights = jnp.zeros_like(state_target)\n",
    "\n",
    "    state_target = state_target.at[0].set(theta)\n",
    "    state_weights = state_weights.at[0].set(1.0)\n",
    "    policy = ActorPolicy(state_target=state_target, state_weights=state_weights)\n",
    "\n",
    "    key, train_state = train_state.split_key()\n",
    "\n",
    "    rng, key = jax.random.split(key)\n",
    "    rngs = jax.random.split(rng, 32)\n",
    "    result_states, eval_infos, dense_states = jax.vmap(\n",
    "        Partial(\n",
    "            eval_actor,\n",
    "            start_state=train_state.config.env.reset(),\n",
    "            train_state=train_state,\n",
    "            policy=policy,\n",
    "        )\n",
    "    )(key=rngs)\n",
    "\n",
    "    return result_states, eval_infos, dense_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7323fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_states, infos, dense_states = eval_model(train_state, jnp.array(-6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3db775",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_states[0, -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6fe3b-772f-49de-b4d3-fdb50ab01a89",
   "metadata": {},
   "source": [
    "## Actually run the actor evals, this will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ActorPolicy(\n",
    "    big_step_size = 0.5,\n",
    "    small_step_size = 0.005,\n",
    "\n",
    "    big_steps = 2048,\n",
    "    small_steps = 2048,\n",
    "\n",
    "    big_post_steps = 32,\n",
    "    small_post_steps = 32,\n",
    "    \n",
    "    state_target = jnp.array([-6.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    state_weights = jnp.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed9e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = env_cls.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize us some actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "(optimized_actions, aux), infos, costs = jax.jit(policy.make_init_carry)(\n",
    "    key=rng,\n",
    "    start_state=start_state,\n",
    "    aux=policy_aux,\n",
    "    net_state=train_state.target_net_state,\n",
    "    train_config=train_state.train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see what the actor thinks would happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "latent_start_state = encode_state(\n",
    "    rng, start_state, train_state.target_net_state, train_state.train_config\n",
    ")\n",
    "expected_latent_states = jax.jit(infer_states)(rng, latent_start_state, optimized_actions, train_state.target_net_state, train_state.train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, expected_latent_states.shape[0])\n",
    "expected_states = jax.jit(\n",
    "    jax.vmap(\n",
    "        Partial(\n",
    "            decode_state,\n",
    "            net_state=train_state.target_net_state,\n",
    "            train_config=train_state.train_config,\n",
    "        )\n",
    "    )\n",
    ")(rngs, expected_latent_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(expected_states[..., 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "\n",
    "media.show_video(\n",
    "    env_cls.host_make_video(\n",
    "        expected_states, train_state.train_config.env_config, dense=False\n",
    "    ).transpose(0, 2, 3, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll out the policy in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanf(carry, key):\n",
    "    \"\"\"Scans to collect a single rollout of physics data.\"\"\"\n",
    "    state, i, policy_carry = carry\n",
    "\n",
    "    rng, key = jax.random.split(key)\n",
    "    action, policy_carry, policy_info = policy(\n",
    "        key=rng,\n",
    "        state=state,\n",
    "        i=i,\n",
    "        carry=policy_carry,\n",
    "        net_state=train_state.target_net_state,\n",
    "        train_config=train_state.train_config,\n",
    "    )\n",
    "    action = jnp.clip(\n",
    "        action,\n",
    "        a_min=train_config.env_config.action_bounds[..., 0],\n",
    "        a_max=train_config.env_config.action_bounds[..., -1],\n",
    "    )\n",
    "    next_state, dense_states = train_config.env_cls.step(\n",
    "        state, action, train_config.env_config\n",
    "    )\n",
    "\n",
    "    return (next_state, i + 1, policy_carry), (\n",
    "        (state, action),\n",
    "        dense_states,\n",
    "        policy_info,\n",
    "    )\n",
    "\n",
    "rng, key = jax.random.split(key)\n",
    "scan_rngs = jax.random.split(rng, train_config.rollout_length - 1)\n",
    "_, ((states, actions), dense_states, policy_info) = jax.lax.scan(\n",
    "    scanf,\n",
    "    (start_state, 0, (optimized_actions, aux)),\n",
    "    scan_rngs,\n",
    ")\n",
    "\n",
    "dense_states = rearrange(dense_states, \"t u s -> (t u) s\")\n",
    "\n",
    "dense_states = jnp.concatenate([start_state[None], dense_states])\n",
    "states = jnp.concatenate([states, start_state[None]])\n",
    "\n",
    "dense_actions = jnp.repeat(actions, 32, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, states.shape[0])\n",
    "latent_states = jax.vmap(\n",
    "    Partial(\n",
    "        encode_state,\n",
    "        net_state=train_state.target_net_state,\n",
    "        train_config=train_state.train_config,\n",
    "    ),\n",
    ")(key=rngs, state=states)\n",
    "\n",
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, dense_states.shape[0])\n",
    "dense_latent_states = jax.vmap(\n",
    "    Partial(\n",
    "        encode_state,\n",
    "        net_state=train_state.target_net_state,\n",
    "        train_config=train_state.train_config,\n",
    "    ),\n",
    ")(key=rngs, state=dense_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, actions.shape[0])\n",
    "latent_actions = jax.vmap(\n",
    "    Partial(\n",
    "        encode_action,\n",
    "        net_state=train_state.target_net_state,\n",
    "        train_config=train_state.train_config,\n",
    "    )\n",
    ")(key=rngs, action=actions, latent_state=latent_states[:-1])\n",
    "\n",
    "\n",
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, dense_actions.shape[0])\n",
    "dense_latent_actions = jax.vmap(\n",
    "    Partial(\n",
    "        encode_action,\n",
    "        net_state=train_state.target_net_state,\n",
    "        train_config=train_state.train_config,\n",
    "    )\n",
    ")(key=rngs, action=dense_actions, latent_state=dense_latent_states[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "inferred_latent_states = infer_states(\n",
    "    key=rng,\n",
    "    latent_start_state=latent_states[0],\n",
    "    latent_actions=latent_actions,\n",
    "    net_state=train_state.target_net_state,\n",
    "    train_config=train_state.train_config,\n",
    "    current_action_i=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = inferred_latent_states - latent_states[1:]\n",
    "diff_norms = jnp.linalg.norm(diffs, ord=1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jnp.mean(diff_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diff_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, inferred_latent_states.shape[0])\n",
    "\n",
    "media.show_video(\n",
    "    env_cls.host_make_video(\n",
    "        # jax.vmap(\n",
    "        #     Partial(\n",
    "        #         decode_state,\n",
    "        #         net_state=train_state.target_net_state,\n",
    "        #         train_config=train_state.train_config,\n",
    "        #     )\n",
    "        # )(key=rngs, latent_state=inferred_latent_states),\n",
    "        dense_states,\n",
    "        env_config=train_state.train_config.env_config,\n",
    "        dense=True,\n",
    "    ).transpose([0, 2, 3, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a scatterplot of all of the latent states and actions the algorithm decided on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.scatter(\n",
    "    dense_latent_states[..., 4],\n",
    "    dense_latent_states[..., 5],\n",
    "    c=range(dense_latent_states.shape[0]),\n",
    "    cmap=\"viridis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optimized_actions[..., 0], optimized_actions[..., 1], c=range(len(optimized_actions)), cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot the achieved final costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(info.plain_infos['final_cost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize what the algorithm did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "\n",
    "video = env_cls.host_make_video(dense_states, env_config).transpose(0, 2, 3, 1)\n",
    "media.show_video(video, fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's investigate what action space actions are available from the start state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 512\n",
    "rng, key = jax.random.split(key)\n",
    "# action_samples = (\n",
    "#     jax.random.ball(rng, d=train_state.train_config.latent_action_dim, p=1, shape=[sample_count])\n",
    "#     * train_state.train_config.action_radius\n",
    "# )\n",
    "\n",
    "action_samples = (\n",
    "    jax.random.ball(\n",
    "        rng, d=train_state.train_config.latent_action_dim, p=1, shape=[4, sample_count]\n",
    "    )\n",
    "    * 1.0\n",
    ") + jnp.array([[-1, 0], [1, 0], [0, -1], [0, 1]])[:, None, :]\n",
    "\n",
    "\n",
    "rng, key = jax.random.split(key)\n",
    "rngs = jax.random.split(rng, [4, sample_count])\n",
    "action_space_actions = jax.vmap(\n",
    "    jax.vmap(\n",
    "        jax.tree_util.Partial(\n",
    "            decode_action,\n",
    "            latent_state=latent_states[12],\n",
    "            net_state=train_state.target_net_state,\n",
    "            train_config=train_state.train_config,\n",
    "        )\n",
    "    )\n",
    ")(rngs, action_samples)\n",
    "\n",
    "# plt.scatter(x=action_samples[..., 0], y=action_samples[..., 1])\n",
    "for i in range(4):\n",
    "    plt.scatter(x=action_space_actions[i, ..., 0], y=action_space_actions[i, ..., 1])\n",
    "\n",
    "# set lims\n",
    "# plt.xlim(-10, 10)\n",
    "# plt.ylim(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
